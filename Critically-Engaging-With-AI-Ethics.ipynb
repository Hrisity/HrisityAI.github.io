{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781c5a8d",
   "metadata": {},
   "source": [
    "**GUID**: 2653889r\n",
    "**GitHub URL**: https://github.com/Hrisity/HrisityAI.github.io.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b5028e",
   "metadata": {},
   "source": [
    "# Critically Engaging with AI Ethics\n",
    "\n",
    "In this lab we will be critically engaging with existing datasets that have been used to address ethics in AI. In particular, we will explore the [**Jigsaw Toxic Comment Classification Challenge**](https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge). This challenge brought to light bias in the data that sparked the [Jigsaw Unintended Bias in Toxicity Classification Challenge](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification). \n",
    "\n",
    "In this lab, we will dig into the dataset ourselves to explore the biases. We will further explore other datasets to expand our thinking about bias and fairness in AI in relation to aspects such as demography and equal opportunity as well as performance and group unawareness of the model. We will learn more about that in the tutorial below.\n",
    "\n",
    "# Task 1: README!\n",
    "\n",
    "This week, coding activity will be minimal, if any. However, as always, you will be expected to incorporate your analysis, thoughts and discussions into your notebooks as markdown cells, so I recommend you start up your Jupyter notebook in advance. As always, **remember**:\n",
    "\n",
    "- To ensure you have all the necessary Python libraries/packages for running code you are recommended to use your environment set up on the **Glasgow Anywhere Student Desktop**.\n",
    "- Start anaconda, and launch Jupyter Notebook from within Anaconda**. If you run Jupyter Notebook without going through Anaconda, you might not have access to the packages installed on Anaconda.\n",
    "- If you run Anaconda or Jupyter Notebook on a local lab computer, there is no guarantee that these will work properly, that the packages will be available, or that you will have permission to install the extra packages yourself.\n",
    "- You can set up Anaconda on your own computer with the necessary libraries/packages. Please check how to set up a new environement in Anaconda and review the minimum list of Python libraries/packages, all discussed in Week 4 lab.\n",
    "- We strongly recommend that you save your notebooks in the folder you made in Week 1 exercise, which should have been created in the University of Glasgow One Drive - **do not confuse this with personal and other organisational One Drives**. Saving a copy of your notebooks on the University One Drive ensures that it is backed up (the first principles of digital preservation and information mnagement).\n",
    "- When you are on the Remote desktop, the `University of Glasgow One Drive` should be visible in the home directory of the Jupyter Notebook. Other machines may require additional set up and/or navigation for One Drive to be directly accessible from Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba3a8e5",
   "metadata": {},
   "source": [
    "# Task 2: Identifying Bias\n",
    "\n",
    "This week we will make use of one of the [Kaggle](https://www.kaggle.com) tutorials and their associated notebooks to learn how to identify different types of bias. Biases can creep in at any stage of the AI task, from data collection methods, how we split/organise the test set, different algorithms, how the results are interpreted and deployed. Some of these topics have been extensively discussed and as a response, Kaggle has developed a course on AI ethics:\n",
    "\n",
    "- Navigate to the [Kaggle tutorial on Identifying Bias in AI](https://www.kaggle.com/code/alexisbcook/identifying-bias-in-ai/tutorial). \n",
    "- In this section we will explore the [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge) to discover different types of biases that might emerge in the dataset. \n",
    "\n",
    "#### Task 2-a: Understanding the Scope of Bias\n",
    "\n",
    "Read through the first page of the [Kaggle tutorial on Identifying Bias in AI] to understand the scope of biases discussed at Kaggle.\n",
    "- How many types of biases are described on the page? \n",
    "- Which type of bias did you know about already before this course and which type was new to you? \n",
    "- Can you think of any others? Create a markdown cell below to discuss your thoughts on these questions.\n",
    "\n",
    "Note that the biases discussed in the tutorial are not an exhaustive list. Recall that biases can exist across the entire machine learning pipeline. \n",
    "\n",
    "- Scroll down to the end of the Kaggle tutorial page and click on the link to the exercise to work directly with a model and explore the data.** \n",
    "\n",
    "#### Task 2-b: Run through the tutorial. Take selected screenshorts of your activity while doing the tutorial.\n",
    "\n",
    "- Discuss with your peer group, your findings about the biases in the data, including types of biases. \n",
    "- Demonstrate your discussion with examples and screenshots of your activity on the tutorial. Present these in your own notebook.\n",
    "\n",
    "Modify the markdown cell below to address the Tasks 2-a and 2-b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf15d2f9",
   "metadata": {},
   "source": [
    "**Markdown for discussing bias**\n",
    "\n",
    "***Task 2-a: Understanding the Scope of Bias***\n",
    "\n",
    "*Six types of bias*\n",
    "\n",
    "1. Historical bias: This one occurs when data is generated in a flawed world state.\n",
    "2. Representation bias: This happens when datasets used to train a model poorly represent the target audience.\n",
    "3. Measurement bias: This takes place when the data accuracy changes across groups; happens primarily if working with variables that are hard to measure, such variables are called proxy.\n",
    "4. Aggregation bias: This happens when groups are not combined suitably or properly. This can lead to a model “that does not perform well for any group or only performs well for the majority group.”\n",
    "5. Evaluation bias: This occurs when evaluating a model with data that does not represent the intended population.\n",
    "6. Deployment bias: This happens when the model is used differently from the problem the model is intended to solve. \n",
    "*Pre-existing Knowledge:*\n",
    "\n",
    "Before the course, I have heard of Historical bias, Representation bias, Deployment bias, and Evaluation bias. New to me were Measurement bias and Aggregation bias.\n",
    "\n",
    "\n",
    "*Other Biases*\n",
    " \n",
    "After doing some research I found out that there are more biases, such as:\n",
    "\n",
    "1. Confirmation Bias: This happens when the selected data somehow confirms pre-existing beliefs or hypotheses. Such things can lead to a model that supports stereotypical beliefs.\n",
    "2. Algorithmic Bias: This type is introduced by the algorithms. Often because of an assumption made by the designer of the algorithm. Hence, the models could be prejudiced towards certain outcomes.\n",
    "\n",
    "***Task 2-b: Run through the tutorial. Take selected screenshorts of your activity while doing the tutorial***\n",
    "\n",
    "During task 2b, my teammates and I completed the Kaggle tutorial.  The code we worked with loads data from the Jigsaw Unintended Bias in Toxicity Classification competition, including comments, where each is given either the label \"toxic\" or \"not toxic\". \n",
    "\n",
    "This exercise aims to educate us on the six different types of bias. It works in the following way; we train the model with real data and practice with identifying bias.\n",
    "\n",
    "First, we’ve successfully loaded the data\n",
    "\n",
    "![Screenshot one](https://cdn.discordapp.com/attachments/1043922557272870993/1179764720664858664/screenshot_1.png?ex=657af884&is=65688384&hm=2ca362d381c8b99f64fe691a92bb0bf0efa97337f92a3c2aa4bbcbb84ede08ae&)\n",
    "\n",
    "Then, we run a code that uses the data to train a simple model. The output provides information on the model’s accuracy on some test data.\n",
    "\n",
    "![Screenshot two](https://cdn.discordapp.com/attachments/1043922557272870993/1179764978681647165/Screenshot_2.png?ex=657af8c1&is=656883c1&hm=0190c7b978555756faefb4ebea189412528f90ab8c73ae6c89d1a4d59214429c&)\n",
    "\n",
    "Trying the model:\n",
    "I have tried out two comments. The first one was “Dogs are stupid”, which as expected was defined as “toxic”, while the comment “I love dogs”, was defined as “not toxic”\n",
    " ![third screenshot](https://cdn.discordapp.com/attachments/1043922557272870993/1179764997908340767/Screenshot_2023-11-15_at_16.32.08.png?ex=657af8c6&is=656883c6&hm=586a00c3c6713bda04900e5ad5dd9fad29f2a2ccf16d8a72278e2306f67fa721&)\n",
    "\n",
    "Here are the top 10 words that are considered most toxic, along with their coefficients.\n",
    "\n",
    "![4th screenshot](https://cdn.discordapp.com/attachments/1043922557272870993/1179765565867446312/Screenshot_2023-11-15_at_16.32.56.png?ex=657af94d&is=6568844d&hm=c79ec102832634d1769f4fae49e83d3b8ea62b234973f096c8238a06aa4065be&)\n",
    "\n",
    "During the tutorials, we learned about the biases in the data, including types of biases.\n",
    "\n",
    "For example, we first, noticed that the model was biased in favour of Christians and against Muslims, and it seems biased in favour of whites and against blacks. This is a sign of bias, as the provided comments are not toxic.\n",
    "We have also noticed that historical bias also occurs. The example showed that comments that refer to Islam are more likely to be classified as toxic. \n",
    "Measurement bias: an error when classifying non-English comments\n",
    "aggregation bias\n",
    "evaluation bias, representation bias and deployment bias: comments from users in the United Kingdom and deployed to users in Australia.\n",
    "\n",
    "![last screenshot](https://cdn.discordapp.com/attachments/1043922557272870993/1179765957665759313/Screenshot_2023-11-15_at_16.47.58.png?ex=657af9ab&is=656884ab&hm=f8dfadf0818f24a662b24b6f95f6cb54e9dc0c8c54e24d37d58584e3d6c86545&)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b38fa2",
   "metadata": {},
   "source": [
    "# Task 3: Large Language Models and Bias: Word Embedding Demo\n",
    "\n",
    "Go to the [embedding projector at tensorflow.org](http://projector.tensorflow.org/). This may take some time to load so be patient! There is a lot of information being visualised. This will take especially long if you select \"Word2Vec All\" as your dataset. The projector provides a visualisation of the langauge language model called **Word2Vec**.\n",
    "\n",
    "This tool also provides the option of visualising the organisation of hand written digits from the MNIST dataset to see how data representations of the digits are clustered together or not. There is also the option of visualising the `iris` dataset from `scikit-learn` with respect to their categories. Feel free to explore these as well if you like.\n",
    "\n",
    "For the current exercise, we will concentrate on exploring the relationships between the words in the **Word2Vec** model. First, select **Word2Vec 10K** from the drop down menu (top lefthand side). This is a reduced version of **Word2Vec All**. You can search for words by submitting them in the search box on the right hand side. \n",
    "\n",
    "#### Task 3.1: Initial exploration of words and relationships\n",
    "\n",
    "- Type `apple` and click on `Isolate 101 ppints`. This reduces the noise. Note how juice, fruit, wine are closer together than macintosh, computers and atari. \n",
    "- Try also words like `silver` and `sound`. What are your observations. Does it seem like words related to each other are sitting closer to each other?\n",
    "\n",
    "#### Task 3.2: Exploring \"Word2Vec All\" for patterns\n",
    "\n",
    "- Try to load \"Word2Vec All\" dataset if you can (this may take a while so be patient!) and explore the word `engineer`, `drummer`or any other occupation - what do you find? \n",
    "- Do you think perhaps there are concerns of gender bias? If so, how? If not, why not? Discuss it with our peer group and present the results in a your notebook.\n",
    "- Why not make some screenshots to embed into your notebook along with your comment? This could make it more understandable to a broader audience. \n",
    "- Do not forget to include attribution to the authors of the Projector demo.\n",
    "\n",
    "Modify the markdown cell below to present your thoughts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0317fe1",
   "metadata": {},
   "source": [
    "**Markdown cell for discussing large language models**\n",
    "\n",
    "# Exploration of Word Relationships in Word2Vec Models\n",
    "\n",
    "## Task 3.1: Initial Observations\n",
    "\n",
    "- **Word:** Apple\n",
    "![apple](https://cdn.discordapp.com/attachments/1043922557272870993/1179813543198072913/Screenshot_2023-11-30_at_15.56.45.png?ex=657b25fc&is=6568b0fc&hm=36323b6e1b4406422e2e58d6a3e6125f374654c3dd7614c5b94fde3fd9c63c4f&)\n",
    "- **Related Words:** Macintosh, fruit, lotus, microsoft, juice, amiga\n",
    "- **Observation:** juice, fruit, and wine are closer together than macintosh, computers and atari.\n",
    "\n",
    "- **Word:** Silver\n",
    "- **Related Words:** gold, copper, iron, bronze, nickel\n",
    "- **Observation:** Metals are closer together than shades/colours yellow and golden, for example.\n",
    "\n",
    "- **Word:** Sound\n",
    "- **Related Words:** noise, sounds, audio, acoustic, listening, quality, records\n",
    "- **Observation:** The results are words related to “sound” \n",
    "\n",
    "## Task 3.2: Word2Vec All and Occupation Analysis\n",
    "\n",
    "- **Occupation Words Explored:** Engineer\n",
    "![engineer screenshot](https://cdn.discordapp.com/attachments/1043922557272870993/1179815875344338974/Screenshot_2023-11-30_at_16.02.45.png?ex=657b2828&is=6568b328&hm=a67ffc528d6c2b9dde19fe499a862f71b14298be8131d5e6a1f9b29bcf3ce474&)\n",
    "\n",
    "- **Related Words:** engineers, scientist, engineering, designer, inventor, mathematician, architect, writer.\n",
    "- **Observation:** The results are mostly professions in the field of engineering. I do not notice gender bias. The results for the word \"teacher\" are similar but in the field of music. However, for the word \"musician\", things changed. I have notice some male names, mostly names of famous people in the field of music. I believe that is a result of the data the model has been trained on.\n",
    "![musician screenshot](https://cdn.discordapp.com/attachments/1043922557272870993/1179815855891173476/Screenshot_2023-11-30_at_16.01.38.png?ex=657b2823&is=6568b323&hm=e6d82624110a621fa1771d7d3700668b9be35a4beaf4c64b0ea5a756cc5a1b04&)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ffeb50",
   "metadata": {},
   "source": [
    "# Task 4: Thinking about AI Fairness \n",
    "\n",
    "So we now know that AI models (e.g. large language models) can be biased. We saw that with the embedding projector already. We discussed in the previous exercise about the machine learning pipeline, how the assessment of datasets can be crucicial to deciding the suitability of deploying AI in the real world. This is where data connects to questions of fairness.\n",
    "\n",
    "- Navigate to the [Kaggle Tutorial on AI Fairness](https://www.kaggle.com/code/alexisbcook/ai-fairness). \n",
    "\n",
    "#### Task 4-a: Topics in AI Fairness\n",
    "Read through the page to understand the scope of the fairness criteria discussed at Kaggle. Just as we dicussed with bias, the fairness criteria discussed at Kaggle is not exhaustive. \n",
    "- How many criteria are described on the page? \n",
    "- Which criteria did you know about already before this course and which, if any, was new to you? \n",
    "- Can you think of any other criteria? Create a markdown cell and note down your discussion with your peer group on these questions.\n",
    "\n",
    "#### Task 4-b: AI fairness in the context of the credit card dataset. \n",
    "We took a tutorial on learning about different ways of measuring the fairness of a machine learning model. During the exercise we trained a few models to approve (or deny) credit card applications and analyse fairness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493fbafc",
   "metadata": {},
   "source": [
    "**Markdown cell for discussing fairness**\n",
    "**Task 4-a: Topics in AI Fairness**\n",
    "\n",
    "***Four fairness criteria***\n",
    "- **Demographic parity**\n",
    "- **Equal opportunity fairness**\n",
    "- **Equal accuracy**\n",
    "- **Group unaware fairness**\n",
    "\n",
    "Before this course I had heard of “Demographic Parity” and \"Equal Opportunity\", the other two are new to me.\n",
    "\n",
    "***Additional Fairness Criteria Discussed***\n",
    "\n",
    "- **Individual Fairness:**   \n",
    "- **Counterfactual Fairness:** \n",
    "- **Causal Reasoning Fairness:**\n",
    "- **Fairness Through Awareness:** \n",
    "\n",
    "***Task 4-b***: AI fairness in the context of the credit card dataset.\n",
    "First, we successfully loaded the data. The dataset contains the following columns: income, number of children, whether the applicant owns a car, and whether the applicant owns a home, for each applicant.\n",
    "\n",
    "![screenshot debit cards 1](https://cdn.discordapp.com/attachments/1043922557272870993/1179809316484685854/Screenshot_2023-11-30_at_12.58.22.png?ex=657b220c&is=6568ad0c&hm=743875599891582e7bf9147883bc2682145894bb36bdbbe989e5795bc7da2a97&)\n",
    "\n",
    "Then, we run the code cell that trains a simple model to approve or deny individuals for a credit card. \n",
    "\n",
    "![screenshot debit card 2](https://cdn.discordapp.com/attachments/1043922557272870993/1179809339469475931/Screenshot_2023-11-30_at_15.22.20.png?ex=657b2212&is=6568ad12&hm=1196681e1b32f7a87bef4b9de0a1c3ca7d6e96b399606d89c17566b6e9da0b3d&)\n",
    "\n",
    "The model:\n",
    "![screenshot of the model](https://cdn.discordapp.com/attachments/1043922557272870993/1179809361783181352/Screenshot_2023-11-30_at_15.26.13.png?ex=657b2217&is=6568ad17&hm=ded7a8fca2978134e97a3bdb0b904ee025c05da7d58e3c92e52afaa47c4b8aae&)\n",
    "\n",
    "Summary:\n",
    "Three types of fairness were covered in the tutorial:\n",
    "“***Demographic parity***: Which group has an unfair advantage, with more representation in the group of approved applicants? (Roughly 50% of applicants are from Group A, and 50% of applicants are from Group B.)\n",
    "***Equal accuracy***: Which group has an unfair advantage, where applicants are more likely to be correctly classified?\n",
    "***Equal opportunity***: Which group has an unfair advantage, with a higher true positive rate?”\n",
    "\n",
    "Overall, with this exercise, we explored different types of fairness. The focus here was on model training. However, as mentioned in the tutorial, in order to get a fair ML system, from the collection of data to the release of a final product every step should be carefully taken into consideration. \n",
    "\n",
    "From the data we worked with, it can be noticed that individuals from Group B have a higher income than those from Group A. Hence, they are more likely to own a car or a home. If we have to decide on a fairness criterion and ways of achieving fairness, knowing this would be invaluable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264c12a",
   "metadata": {},
   "source": [
    "# Task 5: AI and Explainability\n",
    "\n",
    "In this section we will explore the reasons behind decisions that AI makes. While this is really hard to know, there are some approaches developed to know which features in your data (e.g. median_income in the housing dataset we used before) played a more important role than others in determining how your machine learning model performs. One of the many approaches for assessing feature importance is **permutation importance**.\n",
    "\n",
    "The idea behind permutation importance is simple. Features are what you might consider the columns in a tabulated dataset, such as that might be found in a spreadsheet. \n",
    "- The idea of permutation importance is that a feature is important if the performance of your AI program gets messed up by **shuffling** or **permuting** the order of values in that feature column for the entries in your test data. \n",
    "- The more your AI performance gets messed up in response to the shuffling, the more likely the feature was important for the AI model.\n",
    " \n",
    "To make this idea more concrete, read through the page at the [Tutorial on Permutation Importance](https://www.kaggle.com/code/dansbecker/permutation-importance) at Kaggle. The page describes an example to \"predict a person's height when they become 20 years old, using data that is available at age 10\". \n",
    "\n",
    "The page invites you to work with code to calculate the permutation importance of features for an example in football to predict \"whether a soccer/football team will have the \"Man of the Game\" winner based on the team's statistics\". Scroll down to the end of the page to the section \"Your Turn\" where you will find a link to an exercise to try it yourself to calculate the importance of features in a Taxi Fare Prediction dataset.\n",
    "\n",
    "#### Task 1-a: Carry out the exercise, taking screenshots of the exercise as you make progress. Using screen shots and text in your notebook, answer the following question: \n",
    "1. How many features are in this dataset? \n",
    "2. Were the results of doing the exercise contrary to intuition? If yes, why? If no, why not? \n",
    "3. Discuss your results with your peer group.\n",
    "4. Include your screenshots, text, and discyssions in a markdown cell.\n",
    "\n",
    "***Task 5-a***: \n",
    "\n",
    "## There are 7 features considered in the analysis\n",
    "The dataset in the exercise includes the following features:\n",
    "- **’pickup_longitude’**\n",
    "- **’pickup_latitude’**\n",
    "- **’dropoff_longitude’**\n",
    "- **’dropoff_latitude’**\n",
    "- **’passenger_count’**\n",
    "**This makes a total of 5 features initially. Additionally, two, abs_lon_change and abs_lat_change, are created later for analysing the importance of the distance traveled.**\n",
    "\n",
    "***Discussing the Results***\n",
    "\n",
    "The results could be intuitive or contrary to intuition, depending on the initial hypotheses and understanding of the data.\n",
    "\n",
    "\n",
    "\n",
    "#### Task 1-b: Reflecting on Permutation Importance.\n",
    "\n",
    "- Do you think the permutation importance is a reasonable measure of feature importance? \n",
    "- Can you think of any examples where this would have issues? \n",
    "- Discuss these questions in your notebook - describe your example, if you have any, and discuss the issues. \n",
    "\n",
    "In brief, permutation importance is a model inspection technique that has been used in machine learning to evaluate the significance of features in a model; it does not require the retraining of the model, which actually makes it efficient method for assessing feature significance (Scikit-learn documentation, 2023; Christoph Molnar, 2023).\n",
    "\n",
    "In cases like a dataset that predicts real estate prices, features like \"Size of House\" and \"Number of Rooms\" are quite likely to be correlated, which may lead to the technique underestimating the significance of features due to the correlation (Scikit-learn documentation, 2023; Christoph Molnar, 2023). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44978d66",
   "metadata": {},
   "source": [
    "# Task 6: Further Activities for Broader Discussion\n",
    "\n",
    "Apart from the [**Jigsaw Toxic Comment Classification Challenge**](https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge) another challenge you might explore is the [**Inclusive Images Challenge**](https://www.kaggle.com/c/inclusive-images-challenge). Read at least one of the following.\n",
    "\n",
    "- The [announcement of the Inclusive Images Challenge made by Google AI](https://ai.googleblog.com/2018/09/introducing-inclusive-images-competition.html). Explore the [Open Images Dataset V7](https://storage.googleapis.com/openimages/web/index.html) - this is where the Inclusive Images Challenge dataset comes from.\n",
    "- Article summarising [the Inclusive Image Challenge at NeurIPS 2018 conference](https://link.springer.com/chapter/10.1007/978-3-030-29135-8_6)\n",
    "- Explore the [recent controversy](https://www.theverge.com/21298762/face-depixelizer-ai-machine-learning-tool-pulse-stylegan-obama-bias) about bias in relation to [PULSE](https://paperswithcode.com/method/pulse) which, among other things, sharpens blurry images.\n",
    "- Given your exploration in the sections above, what problems might you foresee with [these tasks attempted with the Jigsaw dataset on toxicity](https://link.springer.com/chapter/10.1007/978-981-33-4367-2_81)?\n",
    "\n",
    "There are many concepts (e.g. model cards and datasheets) omitted in discussion above about AI and Ethics. To acquire a foundational knowledge of transparency, accessibility and fairness:\n",
    "\n",
    "- You are welcome to carry out the rest of the [Kaggle course on Intro to AI Ethics](https://www.kaggle.com/learn/intro-to-ai-ethics) to see some ideas from the Kaggle community. \n",
    "- You are welcome to carry out the rest of the [Kaggle tutorial on explainability]( https://www.kaggle.com/learn/machine-learning-explainability) but these are a bit more technical in nature.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca51345e",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this lab, you explored a number of areas that pose challenges with regard to AI and ethics: bias, fairness and explainability. This, and other topics in reposible AI development, is currently at the forefront of the AI landscape. \n",
    "\n",
    "The discussions coming up in the lectures on applications of AI (to be presented by guest lecturers in the weeks to come) will undoubtedly intersect with these concerns. In preparation, you might think, in advance, about **what distinctive questions about ethics might arise in AI applications in law, language, finance, archives, generative AI and beyond**.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15009a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
